{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "import keras \n",
    "from keras.models import Sequential, Model \n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi we understand this can be upsetting driver ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey savani weve fixed this for you and process...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey sanju for us to assist you better kindly h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we understand your concern nikhil we have made...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hey vinay were sorry to hear about the trouble...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Category\n",
       "0  hi we understand this can be upsetting driver ...         1\n",
       "1  hey savani weve fixed this for you and process...         1\n",
       "2  hey sanju for us to assist you better kindly h...         1\n",
       "3  we understand your concern nikhil we have made...         0\n",
       "4  hey vinay were sorry to hear about the trouble...         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Uber_Category_2500.csv\",encoding='latin1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Category</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi we understand this can be upsetting driver ...</td>\n",
       "      <td>1</td>\n",
       "      <td>hi we understand this can be upsetting driver ...</td>\n",
       "      <td>[hi, we, understand, this, can, be, upsetting,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey savani weve fixed this for you and process...</td>\n",
       "      <td>1</td>\n",
       "      <td>hey savani weve fixed this for you and process...</td>\n",
       "      <td>[hey, savani, weve, fixed, this, for, you, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey sanju for us to assist you better kindly h...</td>\n",
       "      <td>1</td>\n",
       "      <td>hey sanju for us to assist you better kindly h...</td>\n",
       "      <td>[hey, sanju, for, us, to, assist, you, better,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we understand your concern nikhil we have made...</td>\n",
       "      <td>0</td>\n",
       "      <td>we understand your concern nikhil we have made...</td>\n",
       "      <td>[we, understand, your, concern, nikhil, we, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hey vinay were sorry to hear about the trouble...</td>\n",
       "      <td>1</td>\n",
       "      <td>hey vinay were sorry to hear about the trouble...</td>\n",
       "      <td>[hey, vinay, were, sorry, to, hear, about, the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Category  \\\n",
       "0  hi we understand this can be upsetting driver ...         1   \n",
       "1  hey savani weve fixed this for you and process...         1   \n",
       "2  hey sanju for us to assist you better kindly h...         1   \n",
       "3  we understand your concern nikhil we have made...         0   \n",
       "4  hey vinay were sorry to hear about the trouble...         1   \n",
       "\n",
       "                                               clean  \\\n",
       "0  hi we understand this can be upsetting driver ...   \n",
       "1  hey savani weve fixed this for you and process...   \n",
       "2  hey sanju for us to assist you better kindly h...   \n",
       "3  we understand your concern nikhil we have made...   \n",
       "4  hey vinay were sorry to hear about the trouble...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [hi, we, understand, this, can, be, upsetting,...  \n",
       "1  [hey, savani, weve, fixed, this, for, you, and...  \n",
       "2  [hey, sanju, for, us, to, assist, you, better,...  \n",
       "3  [we, understand, your, concern, nikhil, we, ha...  \n",
       "4  [hey, vinay, were, sorry, to, hear, about, the...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = data\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_clean['clean'] = df_clean['Tweets'].astype('str') \n",
    "df_clean.dtypes\n",
    "\n",
    "df_clean[\"tokens\"] = df_clean[\"clean\"].apply(tokenizer.tokenize)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85242340, 787050000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WORD2VEC()\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer, important for a parameter of the model\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "\n",
    "#BUILD_VOCAB()\n",
    "\n",
    "w2v_model.build_vocab(df_clean[\"tokens\"], progress_per=1000)\n",
    "\n",
    "\n",
    "#TRAIN()\n",
    "\n",
    "w2v_model.train(df_clean[\"tokens\"], total_examples=w2v_model.corpus_count, epochs=10000, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First defining the X (input), and the y (output)\n",
    "\n",
    "y = data['Category'].values\n",
    "X = np.array(df_clean[\"tokens\"])\n",
    "\n",
    "#And here is the train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 354\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2v_model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape for training set :  (1999, 300) \n",
      "shape for test set :  (500, 300)\n"
     ]
    }
   ],
   "source": [
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_train)])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_test)])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "print ('shape for training set : ',train_vecs_w2v.shape,\n",
    "      '\\nshape for test set : ', test_vecs_w2v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 55,169\n",
      "Trainable params: 55,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_dim=300))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1999 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "1999/1999 [==============================] - 1s 307us/step - loss: 0.4729 - accuracy: 0.8044 - val_loss: 0.3792 - val_accuracy: 0.8400\n",
      "Epoch 2/20\n",
      "1999/1999 [==============================] - 0s 85us/step - loss: 0.3681 - accuracy: 0.8484 - val_loss: 0.3538 - val_accuracy: 0.8560\n",
      "Epoch 3/20\n",
      "1999/1999 [==============================] - 0s 85us/step - loss: 0.3139 - accuracy: 0.8624 - val_loss: 0.3290 - val_accuracy: 0.8980\n",
      "Epoch 4/20\n",
      "1999/1999 [==============================] - 0s 84us/step - loss: 0.2922 - accuracy: 0.8809 - val_loss: 0.3249 - val_accuracy: 0.8800\n",
      "Epoch 5/20\n",
      "1999/1999 [==============================] - 0s 83us/step - loss: 0.2588 - accuracy: 0.8864 - val_loss: 0.3150 - val_accuracy: 0.8940\n",
      "Epoch 6/20\n",
      "1999/1999 [==============================] - 0s 84us/step - loss: 0.2327 - accuracy: 0.9055 - val_loss: 0.3158 - val_accuracy: 0.8980\n",
      "Epoch 7/20\n",
      "1999/1999 [==============================] - 0s 81us/step - loss: 0.2320 - accuracy: 0.9065 - val_loss: 0.3143 - val_accuracy: 0.8960\n",
      "Epoch 8/20\n",
      "1999/1999 [==============================] - 0s 78us/step - loss: 0.2148 - accuracy: 0.9105 - val_loss: 0.3149 - val_accuracy: 0.8980\n",
      "Epoch 9/20\n",
      "1999/1999 [==============================] - 0s 77us/step - loss: 0.2015 - accuracy: 0.9180 - val_loss: 0.3247 - val_accuracy: 0.9000\n",
      "Epoch 10/20\n",
      "1999/1999 [==============================] - 0s 83us/step - loss: 0.1941 - accuracy: 0.9235 - val_loss: 0.3128 - val_accuracy: 0.9080\n",
      "Epoch 11/20\n",
      "1999/1999 [==============================] - 0s 84us/step - loss: 0.1785 - accuracy: 0.9265 - val_loss: 0.2902 - val_accuracy: 0.9200\n",
      "Epoch 12/20\n",
      "1999/1999 [==============================] - 0s 85us/step - loss: 0.1660 - accuracy: 0.9355 - val_loss: 0.3071 - val_accuracy: 0.9200\n",
      "Epoch 13/20\n",
      "1999/1999 [==============================] - 0s 81us/step - loss: 0.1488 - accuracy: 0.9435 - val_loss: 0.3259 - val_accuracy: 0.9140\n",
      "Epoch 14/20\n",
      "1999/1999 [==============================] - 0s 83us/step - loss: 0.1540 - accuracy: 0.9365 - val_loss: 0.3141 - val_accuracy: 0.9320\n",
      "Epoch 15/20\n",
      "1999/1999 [==============================] - 0s 84us/step - loss: 0.1417 - accuracy: 0.9450 - val_loss: 0.3258 - val_accuracy: 0.9260\n",
      "Epoch 16/20\n",
      "1999/1999 [==============================] - 0s 84us/step - loss: 0.1352 - accuracy: 0.9500 - val_loss: 0.2936 - val_accuracy: 0.9300\n",
      "Epoch 17/20\n",
      "1999/1999 [==============================] - 0s 81us/step - loss: 0.1220 - accuracy: 0.9500 - val_loss: 0.3277 - val_accuracy: 0.9420\n",
      "Epoch 18/20\n",
      "1999/1999 [==============================] - 0s 80us/step - loss: 0.1110 - accuracy: 0.9550 - val_loss: 0.3291 - val_accuracy: 0.9340\n",
      "Epoch 19/20\n",
      "1999/1999 [==============================] - 0s 82us/step - loss: 0.1266 - accuracy: 0.9515 - val_loss: 0.3063 - val_accuracy: 0.9300\n",
      "Epoch 20/20\n",
      "1999/1999 [==============================] - 0s 82us/step - loss: 0.1006 - accuracy: 0.9595 - val_loss: 0.3096 - val_accuracy: 0.9460\n",
      "Training Accuracy: 0.9855\n",
      "Testing Accuracy:  0.9460\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_vecs_w2v, y_train, epochs=20, batch_size=50,\n",
    "                   validation_data=(test_vecs_w2v,y_test))\n",
    "loss, accuracy = model.evaluate(train_vecs_w2v, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_vecs_w2v, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78705 words total, with a vocabulary size of 2015\n",
      "Max sentence length is 55\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in X for word in tokens]\n",
    "all_sentence_lengths = [len(tokens) for tokens in X]\n",
    "ALL_VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(ALL_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(all_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### CHANGE THE PARAMETERS HERE #####################################\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 2015 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 55 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2022 unique tokens.\n",
      "(2023, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if sys.path[0] == '':\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data[\"Tweets\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train.tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = w2v_model[word] if word in w2v_model else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## TRAIN AND TEST SET #################################\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test.tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate([convs[0],convs[1],convs[2]],axis=1)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    # Finally, we feed the output into a Sigmoid layer.\n",
    "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
    "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
    "    preds = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 55, 300)      606900      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 53, 128)      115328      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 52, 128)      153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 51, 128)      192128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 17, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 17, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 17, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 51, 128)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 51, 128)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6528)         0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          835712      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            129         dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,903,925\n",
      "Trainable params: 1,297,025\n",
      "Non-trainable params: 606,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1999 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "1999/1999 [==============================] - 7s 3ms/step - loss: 0.4248 - accuracy: 0.8429 - val_loss: 0.2721 - val_accuracy: 0.9040\n",
      "Epoch 2/10\n",
      "1999/1999 [==============================] - 6s 3ms/step - loss: 0.2226 - accuracy: 0.9110 - val_loss: 0.2670 - val_accuracy: 0.9000\n",
      "Epoch 3/10\n",
      "1999/1999 [==============================] - 6s 3ms/step - loss: 0.1610 - accuracy: 0.9420 - val_loss: 0.2844 - val_accuracy: 0.9260\n",
      "Epoch 4/10\n",
      "1999/1999 [==============================] - 5s 3ms/step - loss: 0.0956 - accuracy: 0.9605 - val_loss: 0.2455 - val_accuracy: 0.9380\n",
      "Epoch 5/10\n",
      "1999/1999 [==============================] - 6s 3ms/step - loss: 0.0586 - accuracy: 0.9775 - val_loss: 0.2543 - val_accuracy: 0.9220\n",
      "Epoch 6/10\n",
      "1999/1999 [==============================] - 6s 3ms/step - loss: 0.0452 - accuracy: 0.9845 - val_loss: 0.2472 - val_accuracy: 0.9400\n",
      "Epoch 7/10\n",
      "1999/1999 [==============================] - 6s 3ms/step - loss: 0.0265 - accuracy: 0.9920 - val_loss: 0.2661 - val_accuracy: 0.9420\n",
      "Epoch 8/10\n",
      "1999/1999 [==============================] - 6s 3ms/step - loss: 0.0247 - accuracy: 0.9915 - val_loss: 0.3656 - val_accuracy: 0.9460\n",
      "Epoch 9/10\n",
      "1999/1999 [==============================] - 6s 3ms/step - loss: 0.0239 - accuracy: 0.9915 - val_loss: 0.3903 - val_accuracy: 0.9460\n",
      "Epoch 10/10\n",
      "1999/1999 [==============================] - 5s 3ms/step - loss: 0.0151 - accuracy: 0.9960 - val_loss: 0.3795 - val_accuracy: 0.9440\n",
      "Training Accuracy: 0.9995\n",
      "Testing Accuracy:  0.9440\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_cnn_data, y_train, epochs=10, batch_size=50,\n",
    "                   validation_data=(test_cnn_data,y_test))\n",
    "loss, accuracy = model.evaluate(train_cnn_data, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_cnn_data, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
